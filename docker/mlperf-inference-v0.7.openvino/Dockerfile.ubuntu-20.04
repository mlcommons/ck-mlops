FROM ubuntu:20.04

# News:
#  * 20210525: Grigori updated this container to support the latest CK framework
#              with the latest CK components from ctuning@ai repo
LABEL maintainer="Grigori Fursin <grigori@octoml.ai>"

# Use the Bash shell.
SHELL ["/bin/bash", "-c"]

# Allow stepping into the Bash shell interactively.
ENTRYPOINT ["/bin/bash", "-c"]

# Install known system dependencies and immediately clean up to make the image smaller.
# CK needs: git, wget, zip.
# OpenVINO needs: CMake.
# cv2 needs: libgthread-2.0.
# Python 3.7 needs: libssl, libbz2, libffi.
ENV DEBIAN_FRONTEND=noninteractive
RUN apt update -y\
 && apt install -y apt-utils\
 && apt upgrade -y\
 && apt install -y\
 git wget zip libz-dev\
 cmake\
 libgtk2.0-dev\
 libssl-dev libbz2-dev libffi-dev\
 vim tree\
 && apt clean

# Install Python 3.7 (needed to install TensorFlow 1.15.2 via pip).
# && ./configure --enable-optimizations --enable-shared --with-ensurepip=install \
RUN cd /tmp \
 && wget https://www.python.org/ftp/python/3.7.8/Python-3.7.8.tgz \
 && tar xvf Python-3.7.8.tgz \
 && cd Python-3.7.8 \
 && ./configure --enable-shared --with-ensurepip=install \
 && make -j20 \
 && make altinstall \
 && rm -rf /tmp/*

# Create a non-root user with a fixed group id 1500 and a fixed user id 2000
# (hopefully distinct from any host user id for security reasons).
# See the README for the gory details.
RUN groupadd -g 1500 dvdtg
RUN useradd -u 2000 -g dvdtg --create-home --shell /bin/bash dvdt
USER dvdt:dvdtg
WORKDIR /home/dvdt

# Install Collective Knowledge (CK). Make it group-executable.
ENV CK_REPOS=/home/dvdt/CK_REPOS \
    CK_TOOLS=/home/dvdt/CK_TOOLS \
    PATH=/home/dvdt/.local/bin:${PATH} \
    CPATH=/usr/local/include/python3.7m \
    LD_LIBRARY_PATH=/usr/local/lib \
    CK_CC=gcc \
    CK_PYTHON=python3.7 \
    LANG=C.UTF-8

RUN mkdir -p ${CK_REPOS} ${CK_TOOLS}

# Install CK (CK automation f49f20744aba90e2)
# We need to install new pip and setuptools otherwise there is a conflict 
# with the local CK installation of Python packages ...
RUN export DUMMY_CK=A
RUN ${CK_PYTHON} --version
RUN ${DUMMY_CK} ${CK_PYTHON} -m pip install --ignore-installed pip setuptools wheel --user
RUN ${DUMMY_CK} ${CK_PYTHON} -m pip install pyyaml virtualenv --user
RUN ${DUMMY_CK} ${CK_PYTHON} -m pip install ck --user

# Explicitly create a CK experiment entry, a folder that will be mounted
# (with '--volume=<folder_for_results>:/home/dvdt/CK_REPOS/local/experiment').
# as a shared volume between the host and the container, and make it group-writable.
# For consistency, use the "canonical" uid from ck-analytics:module:experiment.
RUN ck create_entry --data_uoa=experiment --data_uid=bc0409fb61f0aa82 --path=${CK_REPOS}/local\
 && chmod -R g+w ${CK_REPOS}/local/experiment

# Pull CK repositories
RUN ck pull repo:ai

# Use generic Linux settings with dummy frequency setting scripts.
RUN ck detect platform.os --platform_init_uoa=generic-linux-dummy

# Detect C/C++ compiler (gcc).
RUN ck detect soft:compiler.gcc --full_path=`which ${CK_CC}`

# Detect CMake build tool.
RUN ck detect soft --tags=cmake --full_path=`which cmake`

# Detect Python.
RUN ck detect soft --tags=compiler,python --full_path=`which ${CK_PYTHON}`

#-----------------------------------------------------------------------------#
# Step 1. Install Python dependencies (for Model Optimizer and LoadGen).
#-----------------------------------------------------------------------------#
# OpenVINO pre-release strictly requires TensorFlow < 2.0 and NetworkX < 2.4.
RUN ck install package --tags=lib,python-package,tensorflow --force_version=1.15.2
RUN ck install package --tags=lib,python-package,networkx --force_version=2.3.0
RUN ck install package --tags=lib,python-package,defusedxml
# Cython is an implicit dependency of NumPy.
RUN ck install package --tags=lib,python-package,cython
RUN ck install package --tags=lib,python-package,numpy
# test-generator is an implicit dependency of Model Optimizer (not in requirements.txt).
RUN ck install package --tags=lib,python-package,test-generator
# Abseil is a LoadGen dependency.
RUN ck install package --tags=lib,python-package,absl


#-----------------------------------------------------------------------------#
# Step 2. Install C++ dependencies (for Inference Engine and MLPerf program).
#-----------------------------------------------------------------------------#
RUN ck install package --tags=lib,opencv,v3.4.3
RUN ck install package --tags=lib,boost,v1.67.0,from-sourceforge --no_tags=min-for-caffe
RUN ck install package --tags=mlperf,inference,source,dividiti.v0.5-intel
RUN ck install package --tags=lib,loadgen,static
#-----------------------------------------------------------------------------#


#-----------------------------------------------------------------------------#
# Step 3. Install the OpenVINO release to be used for MLPerf Inference v0.7.
#-----------------------------------------------------------------------------#
RUN ck install package --tags=lib,intel,open-model-zoo,2020.2
RUN ck install package --tags=lib,openvino,2020.2
RUN ck compile ai:program:mlperf-inference-v0.5 \
--dep_add_tags.opencv=with-variations
#-----------------------------------------------------------------------------#

#-
#- #-----------------------------------------------------------------------------#
#- # Step 4. Install the first 500 images of the ImageNet 2012 validation dataset
#- # to use as the calibration dataset for image classification models.
#- #-----------------------------------------------------------------------------#
#- RUN ck install package --tags=dataset,imagenet,cal,all.500
#- # Install misc Python dependencies required for calibration.
#- RUN ${CK_PYTHON} -m pip install --user \
#-     nibabel pillow progress py-cpuinfo pyyaml shapely sklearn tqdm xmltodict yamlloader
#- # Install "headless" OpenCV (which doesn't need libsm6, libxext6, libxrender-dev).
#- RUN ck install package --tags=lib,python-package,cv2,opencv-python-headless
#- #-----------------------------------------------------------------------------#
#-
#- 
#- #-----------------------------------------------------------------------------#
#- # Step 5. Install the official ResNet model for MLPerf Inference v0.7
#- # and convert it into the OpenVINO format.
#- #-----------------------------------------------------------------------------#
#- RUN ck install package --tags=model,tf,mlperf,resnet --no_tags=ssd
#- RUN ck install package --tags=model,openvino,resnet50
#- #-----------------------------------------------------------------------------#
#- 
#- 
#- #-----------------------------------------------------------------------------#
#- # Step 6. Install the official MobileNet model for MLPerf Inference v0.5
#- # and convert it into the OpenVINO format. NB: For the open division only.
#- #-----------------------------------------------------------------------------#
#- RUN ck install package --tags=model,tf,mlperf,mobilenet-v1-1.0-224,non-quantized
#- RUN ck install package --tags=model,openvino,mobilenet
#- #-----------------------------------------------------------------------------#
#- 

#-----------------------------------------------------------------------------#
# Step 7. Install the official SSD-MobileNet model for MLPerf Inference v0.7
# and convert it into the OpenVINO format.
#-----------------------------------------------------------------------------#
RUN ck install package --tags=model,tf,ssd-mobilenet,quantized,for.openvino
RUN ck install package --tags=model,openvino,ssd-mobilenet
#-----------------------------------------------------------------------------#


#-----------------------------------------------------------------------------#
# Step 8. Install the COCO 2017 validation dataset (5,000 images).
#-----------------------------------------------------------------------------#
# Download the dataset to the default path. Remove all training annotations (~765 MB).
RUN echo | ck install package --tags=object-detection,dataset,coco.2017,val,original,full\
 && ck virtual env --tags=object-detection,dataset,coco.2017,val,original,full --shell_cmd=\
'rm $CK_ENV_DATASET_COCO_LABELS_DIR/*train2017.json'
# Install Python COCO API.
#RUN ck install package --tags=lib,python-package,cython # already installed
#RUN ck install package --tags=lib,python-package,numpy # already installed
RUN ck install package --tags=lib,python-package,matplotlib
RUN ck install package --tags=tool,coco,api
#-----------------------------------------------------------------------------#


#-----------------------------------------------------------------------------#
# Step 9. Make final preparations to run the OpenVINO program.
#-----------------------------------------------------------------------------#
# Allow the program create tmp files when running under an external user.
RUN chmod -R g+rwx `ck find ai:program:mlperf-inference-v0.5`
#- # The program expects to find val_map.txt in the dataset directory (segfaults otherwise).
#- RUN head -n 500 `ck locate env --tags=aux`/val.txt > `ck locate env --tags=raw,val`/val_map.txt
#- #-----------------------------------------------------------------------------#
#- 
#- 
#- #-----------------------------------------------------------------------------#
#- # Run the OpenVINO program that Intel prepared for MLPerf Inference v0.5
#- # with the quantized ResNet model
#- # on the first 500 images of the ImageNet 2012 validation dataset
#- # using all (virtual) CPU cores.
#- #-----------------------------------------------------------------------------#
#- CMD ["export NPROCS=`grep -c processor /proc/cpuinfo` \
#-  && ck run ai:program:mlperf-inference-v0.5 --skip_print_timers \
#-     --cmd_key=image-classification --env.CK_OPENVINO_MODEL_NAME=resnet50 \
#-     --env.CK_LOADGEN_SCENARIO=Offline --env.CK_LOADGEN_MODE=Accuracy --env.CK_LOADGEN_DATASET_SIZE=500 \
#-     --env.CK_OPENVINO_NTHREADS=$NPROCS --env.CK_OPENVINO_NSTREAMS=$NPROCS --env.CK_OPENVINO_NIREQ=$NPROCS \
#-  && cat /home/dvdt/CK_REPOS/ai/program/mlperf-inference-v0.5/tmp/accuracy.txt"]
#-

#-----------------------------------------------------------------------------#
# Run the OpenVINO program that Intel prepared for MLPerf Inference v0.5
# with the quantized SSD-MobileNet model
# on the first 50 images of the COCO 2017 validation dataset
# using all (virtual) CPU cores.
#-----------------------------------------------------------------------------#
CMD ["export NPROCS=`grep -c processor /proc/cpuinfo` \
 && ck run ai:program:mlperf-inference-v0.5 --skip_print_timers \
    --cmd_key=object-detection --env.CK_OPENVINO_MODEL_NAME=ssd-mobilenet \
    --env.CK_LOADGEN_SCENARIO=Offline --env.CK_LOADGEN_MODE=Accuracy --env.CK_LOADGEN_DATASET_SIZE=50 \
    --env.CK_OPENVINO_NTHREADS=$NPROCS --env.CK_OPENVINO_NSTREAMS=$NPROCS --env.CK_OPENVINO_NIREQ=$NPROCS \
 && cat /home/dvdt/CK_REPOS/ai/program/mlperf-inference-v0.5/tmp/accuracy.txt"]
